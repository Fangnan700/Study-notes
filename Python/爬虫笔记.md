# 第1章 爬虫基础

## 1.1 HTTP基本原理



**URI和URL**

- URI：统一资源标志符
- URL：统一资源定位符

最常使用的是URL（统一资源定位符）

URL的标准格式如下：

```http
scheme://[username:password@]hostname[:port][/path][;parameters][?query][#fragment]
```

- scheme：协议。常用协议有http、https、ftp
- username、password：用户名和密码。某些url需要用户名和密码才能访问
- hostname：主机地址。主机地址可以是域名也可以是IP地址
- port：端口。
- path：路径。所访问资源在服务器中的地址
- parameters：参数。
- query：查询。多个查询之间用&隔开；常见的GET请求中携带的参数如?id=xxx等指的就是query
- fragment：片段。



**HTTP和HTTPS**

- HTTP：超文本传输协议
- HTTPS：在HTTP的基础上加上SSL层。可以理解为HTTP协议的安全版



**HTTP请求过程**

![image-20220705201131331](https://yvling-typora-image-1257337367.cos.ap-nanjing.myqcloud.com/typora/image-20220705201131331.png)

- 客户端向服务器发起请求
- 服务器根据请求进行处理
- 服务器向客户端发送响应
- 客户端解析服务器的响应



**Request 请求**

请求由客户端向服务器发送，包含以下四部分：

- 请求方法（Request Method）
- 请求网址（Request URL）
- 请求头（Request Headers）
- 请求体（Request Body）



请求方法：常用GET和POST两种。

GET请求的参数包含在URL中，数据可以直接被看到。

POST请求的参数和数据以表单的形式发送，安全性更高。



请求头：用于说明服务器需要的附加信息。比较重要的有：

- Cookies：网站识别用户，维持会话不被中断的数据，存储在客户端本地。
- Referer：标识请求来自于哪一个页面。
- User-Agent：带有客户端的操作系统、浏览器的相关信息。写爬虫时加上UA可伪装成浏览器绕过某些反爬限制。



请求体：一般承载POST请求的表单数据。对于GET请求，请求体为空。



**Response 响应**

响应有服务器向客户端发送，包含以下三部分：

- 响应状态码（Response Status Code）
- 响应头（Response Headers）
- 响应体（Response Body）



响应状态码：表示服务器的响应状态。如：200表示正常响应；404表示页面未找到；500表示服务器内部错误等



响应头：包含服务器对请求的应答信息。比较重要的有：

- Content Type：文档类型。指定返回数据的类型
- Server：服务器的相关信息。如名称、版本等
- Set-Cookie：设置Cookies。告诉浏览器将此信息保存并在下次请求时携带此信息。



响应体：响应的正文数据。写爬虫时要解析的就是这部分内容。





## 1.2 Web网页基础



网页三要素：HTML、CSS、JavaScript

- HTML（超文本标记语言）相当于骨架，用于构建网页的基本结构。
- CSS（层叠样式表）相当于皮肤，用于美化网页
- JavaScript（JS）相当于肌肉，用于控制网页的行为与动作



网页的基本结构：一个html标签内嵌套一个head标签和一个body标签。head标签定义网页的配置和引用；body标签定义网页的正文。





## 1.3 爬虫的基本原理



爬虫的定义：获取网页并提取、解析、保存网页信息的自动化程序。

操作流程：

- 使用urllib、requests等库实现模拟浏览器向服务器发送请求
- 接受服务器的响应，解析、提取响应中的数据
- 将提取出的数据保存至本地







## 1.4 Session和Cookie



静态网页：由HTML代码编写，加载速度快，但不能根据url的变化来展示变化的动态内容。

动态网页：由JSP、PHP等脚本语言编写，可以动态解析url中参数的变化，关联数据库从而展示变化的动态内容。

无状态HTTP：HTTP协议对事物处理没有记忆能力，即一次请求响应结束后，服务器不会记录客户端的任何信息。



Session：存储在服务器端的客户端用户信息

Cookie：存储在客户端的用户凭据

每次请求时，若携带由客户端的Cookie信息，服务器就会将Cookie与Session进行比对，判断当前客户端是否处于登录状态



根据以上原理，写爬虫时，将已经存在从Cookie放在请求头中，可以直接请求登录而不用模拟登录。







## 1.5 代理的基本原理



代理（proxy），即代理服务器。可以理解为网络信息的中转站。



代理的作用：

- 突破自身IP的访问限制
- 访问某些单位、团体的内部资源
- 提高访问速度
- 隐藏真实IP



代理的分类：

- FTP代理：主要用于访问FTP服务器，进行资源的上传和下载。一般使用21、2121端口。
- HTTP代理：主要用于访问网页。一般使用80、8080端口。
- SSL/TLS代理：主要用于访问加密网站。一般使用443端口。
- SOCKS代理：只用于传递数据包，不关系具体协议。一般使用1080端口。
- Telnet代理：主要用于Telnet远程控制。一般使用23端口。







## 1.6 多线程和多进程的基本原理



- 进程：一个进程就是一个独立运行的程序单位
- 线程：一个线程就是一个独立运行的任务
- 多线程：多线程就是一个进程中同时执行多个任务，即同时执行多个线程

进程是线程的集合，即进程是由一个或多个线程组成的。线程是操作系统进行运算的最小单位。



在计算机中，执行一个任务是通过处理器运行一条条指令来实现的。

- 并发：多个线程对应的多条指令被快速轮换地执行
- 并行：同一时刻，有多条指令在多个处理器上同时执行







# 第2章 基本库的使用

## 2.1 urllib的使用



urllib是python内置的HTTP请求库，包含以下四个模块：

- request：最基本的http请求模块，可模拟发送请求。
- error：异常处理模块。
- parse：工具模块。提供拆分、解析、合并等url处理功能。
- robotparser：识别网站的robots.txt文件。



### 2.1.1 发送请求



使用urllib库的request模块，只需要传入url以及额外的参数就可以轻松的发送请求。



**使用urlopen：**

```python
import urllib.request

resp = urllib.request.urlopen("https://www.python.org")
print(type(resp))
```

![image-20220705223642065](https://yvling-typora-image-1257337367.cos.ap-nanjing.myqcloud.com/typora/image-20220705223642065.png)

得到的响应是一个HTTPResponse类型的对象。

这个对象包括read、readinto、getheader、getheaders等方法，以及msg、version、stautus、reason等属性。

将这个对象赋值给resp之后，就可以调用对象里的方法和属性。

 比如：调用read方法可以获取网页的内容。

```python
print(resp.read())
```



urlopen的参数使用：

urlopen方法的API：

```python
urllib.request.urlopen(url, data=None, [timeout], cafile=None, capath=None, cadefault=False, context=None)
```



- data参数：添加该参数是需要使用 **bytes方法** 将参数转化为字节流编码格式（bytes类型），传递该参数后，请求方式变为post。

```python
import urllib.request
import urllib.parse

#先使用urllib.parse模块中的urlcode方法将字典转换为字符串，再使用bytes方法将字符串按‘utf-8’格式转换为字节流，最后赋值给data
data = bytes(urllib.parse.urlencode({'name': 'kali'}), encoding='utf-8')

#使用urllib.request模块中的urlopen方法发起请求并传递参数data，将获取的响应对象赋值给response
response = urllib.request.urlopen('https://www.httpbin.org/post', data=data)

#调用response对象中的read方法获取响应的页面内容并使用decode方法按‘utf-8’格式进行解码
print(response.read().decode('utf-8'))
```

![image-20220706104325367](https://yvling-typora-image-1257337367.cos.ap-nanjing.myqcloud.com/typora/image-20220706104325367.png)



- timeout参数：用于设置超时时间，超出该事件还未得到响应则抛出异常。

```python
import urllib.request

#发起请求时，添加参数timeout=0.1
response = urllib.request.urlopen('https://www.httpbin.org/get', timeout=0.1)
print(response.read())
```

![image-20220706110436667](https://yvling-typora-image-1257337367.cos.ap-nanjing.myqcloud.com/typora/image-20220706110436667.png)

该异常属于urllib.error模块，错误原因时超时未响应。

使用try except语句实现当一个网页长时间未响应时执行某些语句：

```python
import socket	#套接字客户端模块，用于建立信息通道
import urllib.request
import urllib.error

try:
    response = urllib.request.urlopen('https://www.httpbin.org/get', timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('time out!')
```





















## 2.2 requests的使用

## 2.3 正则表达式

## 2.4 httpx的使用

## 2.5 基础爬虫案例

































